---
title: "Project1 Yu Tong yt2594"
output:
  pdf_document: default
  html_notebook: default
---
In this project, we are going to explore the 'story' behind the inauguation speeches of american presidents.

# Phase 1: Data Preparation
```{r, echo=FALSE, warning = FALSE, message=FALSE}
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")

```
Step 1: Create a data frame that contains information of speeches.
```{r}
# Required packages are already loaded
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page) # cited from the "wk2-Tutorial-TextMining"
dat = as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] 
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list$Date <- dat[1:58]
inaug.list$urls <- inaug$urls
inaug.list$fulltext=NA
for(i in seq(nrow(inaug.list))) {
  text <- read_html(inaug.list$urls[i]) %>% 
    html_nodes(".displaytext") %>% 
    html_text() 
  inaug.list$fulltext[i]=text
  filename <- paste0("../data/InauguralSpeeches/", 
                     "inaug",
                     inaug.list$File[i], "-", 
                     inaug.list$Term[i], ".txt")
  sink(file = filename) %>% 
  cat(text)  
  sink() 
}
write.csv(inaug.list,file="/Users/warabe/Documents/GitHub/Spring2018-Project1-Tonyfer/output/wholeinauglist.csv")
```
The first 10 lines of datafrme 'inaug.list' are:
```{r}
head(inaug.list, 10)
```
Step 2: Prepare data for wordcloud and tf-idf
```{r, echo=FALSE, message=FALSE}
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
```
Read in speeches:
```{r}
folder.path= "/Users/warabe/Documents/GitHub/Spring2018-Project1-Tonyfer/data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(speeches, 6, nchar(speeches)-4)
ff.all<-Corpus(DirSource(folder.path))
```
Processing speeches:remove extra white space, convert all letters to the lower case, remove [stop words], removed empty words due to formatting errors, and remove punctuation. 
```{r}
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
tdm.all<-TermDocumentMatrix(ff.all)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
```

# Phase 2: Data Visualization



Step 1: Create wordcloud for all the speeches:
```{r}
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=c("4", "7", "10"))
```
We can see from this wordcloud that the top three most frequent words are "will", "people" and "government", it shows that the inauguation speeches are mostly about the promise that presidents made to the public.

Step 2: Interactive visualize important words in individual speeches
```{r, warning=FALSE, message=FALSE}
library(shiny)
```
```{r, warning=FALSE}
shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1', speeches, selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches, selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3, min = 20, 
                              max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),

    server = function(input, output, session) {
      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech1))],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech1))],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech2))],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech2))])
      })

      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=c(4,5,6), 
            main=input$speech2)
      })
    },

    options = list(height = 600)
)
```
# Phase 3: Use clustering analysis to cluster single words.
Because in one speech, the speaker always use some words that refer to similiar meaning, we can use clustering analysis to find out similiar words.
```{r}
tdm_removed<-removeSparseTerms(tdm.all, 0.25) # Because there are totally 9403 words, it's too much for the plot, we just choose some frequent term of them. 
mydata <- as.data.frame(as.matrix(tdm_removed)) 
mydata.scale<-scale(mydata)                        
d<-dist(mydata.scale,method="euclidean")        
fit <- hclust(d, method="ward.D")                
plot(fit) 
```
This plot makes sences because the word "will" meant to be the most common word among all the speeches because the speeches are all about what the prisident and government "will" do for the public. And other words that cluster to same area bunch are always come up together. 


# Phase 4: Sentimental Analysis of Each Speech
Step 1: Create sentences list
```{r, warning=FALSE}
sentence.list <- NULL
for(i in 1:nrow(inaug.list)){
  sentences <- sent_detect(inaug.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    #colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(inaug.list[i,-ncol(inaug.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
sentence.list <- sentence.list%>%filter(!is.na(word.count)) 
```
Step 2: Clustering of emotions

Since we already get the emotion of each sentence. We can do clustering for the speeches.
```{r, echo =FALSE}
sel.comparison=c("DonaldJTrump","JohnMcCain", "GeorgeBush", "MittRomney", "GeorgeWBush",
                 "RonaldReagan","AlbertGore,Jr", "HillaryClinton","JohnFKerry", 
                 "WilliamJClinton","HarrySTruman", "BarackObama", "LyndonBJohnson",
                 "GeraldRFord", "JimmyCarter", "DwightDEisenhower", "FranklinDRoosevelt",
                 "HerbertHoover","JohnFKennedy","RichardNixon","WoodrowWilson", 
                 "AbrahamLincoln", "TheodoreRoosevelt", "JamesGarfield", 
                 "JohnQuincyAdams", "UlyssesSGrant", "ThomasJefferson",
                 "GeorgeWashington", "WilliamHowardTaft", "AndrewJackson",
                 "WilliamHenryHarrison", "JohnAdams")
```

```{r, echo = T, fig.width=6, fig.height=6}
presid.summary=tbl_df(sentence.list)%>%
  filter(File%in%sel.comparison)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)
for(i in 2:9){
  presid.summary[,i][is.na(presid.summary[,i])] = 0
}
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
              4)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```
# Phase 5: Use topic model to analyze speeches
Instead of analyze every three sentences of the speeches, I use topic model to analyze the wholie speech.

Step 1: Data preparation
```{r}
docs <- Corpus(VectorSource(inaug.list$fulltext))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument)
```
Step 2: Use LDA(Latent Dirichlet Allocation) analyze the topic of speeches
```{r}
dtm <- DocumentTermMatrix(docs)
dtm  <- dtm[rowTotals> 0, ]
```
```{r}
burnin <- 5000
iter <- 2500
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 15
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))
```
```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```
Step 3: Clustering of topics
```{r, fig.width=3.3, fig.height=6.6}
par(mar=c(1,1,1,1))
topic.summary=tbl_df(inaug.list.df)%>%
              select(File, Economy:Legislation)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
```


Step 4: Clustering thespeeches based on the topics
```{r, message = FALSE, fig.width=6, fig.height=6}
presid.summary=tbl_df(inaug.list.df)%>%
  filter(File%in%sel.comparison)%>%
  select(File, Economy:Legislation)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
              4)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=T)
```


# Phase 6: Find the nearest neighberhood of a speech
By using tf-idf, we can compute the cosine distance and the euclidean distance of every two speeches. Then we can analyze the similiarity of the speech and its neighbors.

Step 1: Compute tf-idf
```{r, warning=FALSE}
dtm <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
```


Step 2: Compute two kinds of ditances
```{r}
mat <- as.matrix(dtm)
i <- 1:2
j <- 1:2
distance.sqr <- 1:2
dis.sqr <- data.frame(i,j,distance.sqr)
for(i in 1:58){
  for(j in 1:58){
     x <- mat[i, ]
     y <- mat[j, ]
     d <- sqrt(sum((x-y)^2))
     dis.sqr[(i-1)*58 + j, 1] <- i
     dis.sqr[(i-1)*58 + j, 2] <- j
     dis.sqr[(i-1)*58 + j, 3] <- d
  }
}
i <- 1:2
j <- 1:2
distance.cos <- 1:2
dis.cos <- data.frame(i,j,distance.cos)
for(i in 1:58){
  for(j in 1:58){
     x <- mat[i, ]
     y <- mat[j, ]
     d <- sum(x*y)/(sqrt(sum(x^2)))/(sqrt(sum(y^2)))
     dis.cos[(i-1)*58 + j, 1] <- i
     dis.cos[(i-1)*58 + j, 2] <- j
     dis.cos[(i-1)*58 + j, 3] <- d
  }
}
```
Step 3: Define a function 'get_query' to output the nearest neighborhoods of document.
```{r}
get_query <- function(president, term, distance){
  sort.ina <- inaug.list[order(inaug.list$President), c(1,3,4,5,6)]
  t = which(sort.ina$President == president)[term]
  if(distance == "sqr"){
    sub <- dis.sqr[dis.sqr$i == t,]
    sub <- sub[order(sub$distance.sqr, decreasing = F), ]
    return(sort.ina[sub$j[1:6], ])
  }
  if(distance == "cos"){
    sub <- dis.cos[dis.cos$i == t,]
    sub <- sub[order(sub$distance.cos, decreasing = T), ]
    return(sort.ina[sub$j[1:6], ])
  }
}
```
For eample,
```{r, warning=FALSE}
get_query("Abraham Lincoln", 1, "cos")
get_query("George W. Bush", 2, "cos")
get_query("George W. Bush", 1, "cos")
get_query("Woodrow Wilson", 2, "cos")
get_query("Franklin Pierce", 1, "cos")
get_query("Harry S. Truman", 1, "cos")

```
As the examples and the clustering plots inthe previous part of this notebook, we can find out that the speeches that close to each other are always happend on similiar situation of america, no matter in economics, freedom, equality and so on. For example, we can see from the cluster plot that the presidents in the same cluster are mostly from the same time period, that is because in same time period, the background of America is always similiar.

But for the speech of George W. Bush, we can find that the nearest neighbor are from prisidents of 1800s, this might because that he is more like an "old school" president.

So next time when a new speech comes, we can get to know the appropriate situation that America is at by analyzing the topics or the tf-idf of the words.


















